## 运行MapReduce作业

### 提交作业

- 向`jobtracker`请求一个新的作业ID
- 检查作业的输出说明
- 计算作业的输出划分
- 将运行做作业所需要的资源复制到一个以作业ID号命名的目录中
- 通知`jobtracker`准备执行

### 作业的初始化

将此调用放入一个内部的队列中，交由作业调度器进行调度，并对其初始化

### 任务分配器

`TaskTracer`执行一个简单的循环，定期发送心跳方法调用`Jobtracer`，作为两者之间的消息通道

### 任务的执行

`tasktracker`本地化作业的jar文件，将他从共享文件系统复制到`tasktracker`所在的文件系统，将应用程序所需要的全部文件从分布式缓存复制到`tasktracker`本地磁盘；新建`TaskRunner`实例来运行任务

**流和管道**

运行用户提供的可执行程序并与之通信

## 失败

**子任务失败**
**JVM突然退出**
**tasktracker失败**：
由于崩溃或者运行过于缓慢而失败，`jobtracker`将他从等待任务的池中移除，安排此`tasktracker`上已经运行并成功的map任务返回
**`jobtracker`失败**：单点故障

## 作业的调度

按照作业的体骄傲顺序来运行，先进先出的调度算法

**公平调度器**

让每个用户公平的共享集群，如果只有一个作业在运行，它会得到整个集群的资源。随着提交的作业增加，空闲的任务槽会以这种方式分配

## shuffle和排序

![shuffle](https://i.stack.imgur.com/aIGRQ.png)

每个map task都有一个内存缓冲区，存储着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式存放到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task来拉数据。 

1. 在map task执行时，它的输入数据来源于HDFS的block，当然在MapReduce概念中，map task只读取split
2. 在经过mapper的运行后，我们得知mapper的输出是这样一个key/value对
3. 这个内存缓冲区是有大小限制的， 当map task的输出结果很多时，就可能会撑爆内存，所以需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘写数据的过程被称为Spill
4.  每次溢写会在磁盘上生成一个溢写文件，如果map的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个溢写文件存在

![reduce](http://dl.iteye.com/upload/attachment/456527/608c7e08-896d-3697-a57e-a8ca60cf79ea.jpg)

1. Copy过程，简单地拉取数据
2. Merge阶段
3. Reducer的输入文件

## 任务的执行